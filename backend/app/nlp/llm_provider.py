"""
LLM Provider â€” Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸Ñ Ð½Ð°Ð´ LLM Ð´Ð»Ñ Ð»Ñ‘Ð³ÐºÐ¾Ð¹ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸.
Ð¡ÐµÐ¹Ñ‡Ð°Ñ: Ollama (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾, Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾).
ÐŸÐ¾Ñ‚Ð¾Ð¼: OpenAI API (Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡ â€” Ð¸ Ð²ÑÑ‘ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚).
"""
from abc import ABC, abstractmethod
from app.config import get_settings


class LLMProvider(ABC):
    """Abstract LLM interface."""

    @abstractmethod
    async def generate(self, system_prompt: str, user_message: str, history: list[dict] = None) -> str:
        """Generate a response given system prompt, user message, and optional history."""
        ...


class OllamaProvider(LLMProvider):
    """Local Ollama LLM provider (free, runs on CPU)."""

    def __init__(self):
        settings = get_settings()
        self.model = settings.OLLAMA_MODEL
        self.base_url = settings.OLLAMA_BASE_URL

    async def generate(self, system_prompt: str, user_message: str, history: list[dict] = None) -> str:
        try:
            import ollama
            messages = [{"role": "system", "content": system_prompt}]
            if history:
                messages.extend(history)
            messages.append({"role": "user", "content": user_message})

            client = ollama.AsyncClient(host=self.base_url)
            response = await client.chat(model=self.model, messages=messages)
            return response["message"]["content"]
        except Exception as e:
            return f"Ð˜Ð·Ð²Ð¸Ð½Ð¸, Ñ ÑÐµÐ¹Ñ‡Ð°Ñ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ (LLM Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {type(e).__name__}). ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¿Ð¾Ð·Ð¶Ðµ!"


class FallbackProvider(LLMProvider):
    """
    Rule-based fallback when no LLM is available.
    Provides basic responses without AI.
    """

    async def generate(self, system_prompt: str, user_message: str, history: list[dict] = None) -> str:
        msg_lower = user_message.lower()

        if any(w in msg_lower for w in ["Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹", "hello", "hi"]):
            return "ÐŸÑ€Ð¸Ð²ÐµÑ‚! ðŸ‘‹ Ð¯ Ñ‚Ð²Ð¾Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð¿Ð¾ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ°Ð¼. Ð¡Ð¿Ñ€Ð¾ÑÐ¸ Ð¼ÐµÐ½Ñ Ð¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐµ, Ð¿Ð¾Ð¿Ñ€Ð¾ÑÐ¸ ÑÐ¾Ð²ÐµÑ‚ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ð¾Ð±Ð¾Ð»Ñ‚Ð°ÐµÐ¼ Ð¾ Ñ‚Ð²Ð¾Ð¸Ñ… Ñ†ÐµÐ»ÑÑ…!"

        if any(w in msg_lower for w in ["ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ðº", "Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ", "ÐºÐ°Ðº Ð´ÐµÐ»Ð°"]):
            return "Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ, Ð¿ÐµÑ€ÐµÐ¹Ð´Ð¸ Ð½Ð° ÑÐºÑ€Ð°Ð½ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°. Ð¢Ð°Ð¼ Ñ‚Ñ‹ ÑƒÐ²Ð¸Ð´Ð¸ÑˆÑŒ ÑÐµÑ€Ð¸Ð¸, Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¸ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸! ðŸ“Š"

        if any(w in msg_lower for w in ["ÑÐ¾Ð²ÐµÑ‚", "Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†", "Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ", "Ð¿Ð¾Ð¼Ð¾Ð³"]):
            return ("Ð’Ð¾Ñ‚ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ¾Ð²ÐµÑ‚Ð¾Ð²:\n"
                    "1. ðŸŽ¯ ÐÐ°Ñ‡Ð½Ð¸ Ñ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐµÐº â€” 2 Ð¼Ð¸Ð½ÑƒÑ‚Ñ‹ Ð² Ð´ÐµÐ½ÑŒ\n"
                    "2. ðŸ”— ÐŸÑ€Ð¸Ð²ÑÐ¶Ð¸ Ð½Ð¾Ð²ÑƒÑŽ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÑƒ Ðº ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹\n"
                    "3. ðŸ“… Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐ¹ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¸ Ð² Ð¾Ð´Ð½Ð¾ Ð¸ Ñ‚Ð¾ Ð¶Ðµ Ð²Ñ€ÐµÐ¼Ñ\n"
                    "4. ðŸ† ÐžÑ‚Ð¼ÐµÑ‡Ð°Ð¹ ÑÐ²Ð¾Ð¸ ÑƒÑÐ¿ÐµÑ…Ð¸, Ð´Ð°Ð¶Ðµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ðµ!")

        if any(w in msg_lower for w in ["Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†", "Ð½Ðµ Ñ…Ð¾Ñ‡Ñƒ", "Ð»ÐµÐ½ÑŒ", "ÑÐ»Ð¾Ð¶Ð½Ð¾", "Ñ‚Ñ€ÑƒÐ´Ð½Ð¾"]):
            return ("Ð¯ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑŽ, Ð±Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ»Ð¾Ð¶Ð½Ð¾. ÐŸÐ¾Ð¼Ð½Ð¸:\n"
                    "ðŸ’ª Ð”Ð°Ð¶Ðµ 1% ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ â€” ÑÑ‚Ð¾ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð·Ð° Ð³Ð¾Ð´\n"
                    "ðŸŒ± ÐÐµ Ð±Ð¾Ð¹ÑÑ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ð·Ð°Ð½Ð¾Ð²Ð¾ â€” ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ ÑˆÐ°Ð½Ñ\n"
                    "â­ Ð¢Ñ‹ ÑƒÐ¶Ðµ Ð¼Ð¾Ð»Ð¾Ð´ÐµÑ†, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑˆÑŒÑÑ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼!")

        if any(w in msg_lower for w in ["Ð´Ð¾Ð±Ð°Ð²", "Ð½Ð¾Ð²Ð°Ñ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ°", "ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ"]):
            return "Ð§Ñ‚Ð¾Ð±Ñ‹ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÑƒ, Ð½Ð°Ð¶Ð¼Ð¸ ÐºÐ½Ð¾Ð¿ÐºÑƒ '+' Ð½Ð° Ð³Ð»Ð°Ð²Ð½Ð¾Ð¼ ÑÐºÑ€Ð°Ð½Ðµ. Ð’Ñ‹Ð±ÐµÑ€Ð¸ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ Ð¸ Ð²Ñ€ÐµÐ¼Ñ â€” Ñ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð´Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ! âž•"

        return ("Ð¯ Ñ‚Ð²Ð¾Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð¿Ð¾ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ°Ð¼! Ð’Ð¾Ñ‚ Ñ‡Ñ‚Ð¾ Ñ ÑƒÐ¼ÐµÑŽ:\n"
                "ðŸ“Š Ð Ð°ÑÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ\n"
                "ðŸ’¡ Ð”Ð°Ñ‚ÑŒ ÑÐ¾Ð²ÐµÑ‚ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ°Ð¼\n"
                "ðŸ’ª ÐŸÐ¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸ÐµÐ¹\n"
                "ðŸ†• ÐŸÐ¾Ð´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¸\n\n"
                "ÐŸÑ€Ð¾ÑÑ‚Ð¾ ÑÐ¿Ñ€Ð¾ÑÐ¸!")


class OpenAIProvider(LLMProvider):
    """
    OpenAI API provider â€” for future migration.
    Set OPENAI_API_KEY env variable and change config to use this.
    """

    async def generate(self, system_prompt: str, user_message: str, history: list[dict] = None) -> str:
        # Placeholder for future migration
        # import openai
        # client = openai.AsyncOpenAI()
        # messages = [{"role": "system", "content": system_prompt}]
        # if history: messages.extend(history)
        # messages.append({"role": "user", "content": user_message})
        # response = await client.chat.completions.create(model="gpt-4", messages=messages)
        # return response.choices[0].message.content
        raise NotImplementedError("OpenAI provider not configured. Set OPENAI_API_KEY.")


def get_llm_provider() -> LLMProvider:
    """Factory: returns the appropriate LLM provider."""
    try:
        import ollama
        # Try to check if Ollama is running
        provider = OllamaProvider()
        return provider
    except ImportError:
        return FallbackProvider()

